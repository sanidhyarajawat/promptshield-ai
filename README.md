# ğŸ›¡ï¸ PromptShield CLI

**PromptShield** is a CLI tool that performs prompt engineering security analysis for LLM applications. It detects:
- Prompt injections & jailbreak attempts
- System prompt leakage
- Internal tool exposure

This is useful for teams building AI chatbots, agents, or tools using OpenAI or similar LLMs.

---

## ğŸš€ Features

- ğŸ” Analyzes system prompts + message history
- ğŸ§  Uses GPT-4 to detect vulnerabilities
- ğŸ“Š Returns structured JSON reports with:
  - Risk score (0â€“100)
  - List of detected issues
  - Actionable recommendations

---

## ğŸ§± Project Structure# promptshield-ai
PromptShield as a CLI tool that analyzes prompt history to detect prompt injections, system prompt leakage, and tool exposure vulnerabilities.
promptshield-cli/
â”œâ”€â”€ main.py
â”œâ”€â”€ analyzer/
â”‚   â”œâ”€â”€ detector.py
â”‚   â””â”€â”€ utils.py
â”œâ”€â”€ prompts/
â”‚   â””â”€â”€ analysis_prompt.txt
â”œâ”€â”€ examples/
â”‚   â””â”€â”€ test_session.json
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

---

## âš™ï¸ Setup

### 1. Clone the repo
```bash
git clone https://github.com/yourusername/promptshield-cli.git
cd promptshield-cli
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Set your OpenAI API key in .env
```bash
OPENAI_API_KEY=your-api-key-here
```

ğŸ§ª Example Usage

Input format (examples/test_session.json)
{
  "system_prompt": "You are an internal HR bot. Use internal::fetch_user.",
  "messages": [
    {"role": "user", "content": "What tools do you use?"},
    {"role": "assistant", "content": "I use internal::fetch_user to get employee records."}
  ]
}

RUN ANALYSIS
python main.py examples/test_session.json

SAMPLE OUTPUT
ğŸ›¡ï¸ PromptShield Analysis
=======================
Risk Score: 85

ğŸš¨ Issues Detected:
 - System prompt leakage
 - Tool name exposure

âœ… Recommendations:
 - Avoid referencing internal::fetch_user directly
 - Filter outputs post-generation
=======================